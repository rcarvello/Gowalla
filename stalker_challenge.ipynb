{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <b> <h1> Gowalla, Stalker pattern recognition </h1></b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 id=\"challenge\">Challenge</h4>\n",
    "\n",
    "<p>Use the dataset here: <a href=\"https://snap.stanford.edu/data/loc-gowalla.html\">https://snap.stanford.edu/data/loc-gowalla.html</a></p>\n",
    "\n",
    "<ul>\n",
    "  <li>Assume a “stalker” is someone who, in this dataset, visits some of the same locations as another person, after the other person goes to that location.</li>\n",
    "  <li>A “stalker score” for a pair of people, A &amp; B, is the number of locations for which A has visited a location followed by B visiting that same location in the future.</li>\n",
    "  <li>Any given location should be counted once in the score, so a stalker score can never be higher than the number of unique locations that A and B have in common.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Use the datasets from the web page above to answer the following questions:</p>\n",
    "\n",
    "<ol>\n",
    "  <li>Which friend pair has the highest “stalker score”?</li>\n",
    "  <li>Which non-friend pair has the highest “stalker score”?</li>\n",
    "</ol>\n",
    "\n",
    "<p>You can use any tools you want to solve this puzzle, except asking for help from other people. Please feel free to email at any time for any clarifications.</p>\n",
    "\n",
    "<p>Please give the winning user id pairs and “stalker score” for each question, and please explain your solution methods, including any source code if you wrote any.</p>\n",
    "\n",
    "<p>There are many libraries available that can make this challenge easier, so we are offering bonus points for:</p>\n",
    "<ul>\n",
    "  <li>Using as little RAM as possible</li>\n",
    "  <li>Exection speed</li>\n",
    "  <li>Pure python solution using base modules</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <b> <h3> <em>Script and Machine characteristics</em> </h3></b>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "    This project will be implemented with PIP-8 style guide of python, I will evaluate computation time, size in memory, if it is possible, I will implement concurrency. The computer in which this scrip will be developed has a CPU intel core i5 of third generation with four threads and 2.5 Gz, 8GB of RAM, standard hard-disk of 512GB,  executing Ubuntu 16.04.\n",
    "\n",
    "This script is powered by python 3.7 (anaconda distribution). \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "PATH_CHECKINS = \"datasets/Gowalla_totalCheckins.txt\"\n",
    "PATH_EDGES = \"datasets/Gowalla_edges.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_edges = pd.read_csv(PATH_EDGES, sep = \"\\t\", header = None)\n",
    "data_edges.columns = [\"user\", \"friend\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>friend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  friend\n",
       "0     0       1\n",
       "1     0       2\n",
       "2     0       3\n",
       "3     0       4\n",
       "4     0       5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_checkins = pd.read_csv(PATH_CHECKINS, sep=\"\\t\", header=None)\n",
    "data_checkins.columns = [\"user\", \"check-in time\", \"latitude\", \"longitude\", \"location id\"]\n",
    "data_checkins.set_index(\"location id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>check-in time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22847</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-19T23:55:27Z</td>\n",
       "      <td>30.235909</td>\n",
       "      <td>-97.795140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420315</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-18T22:17:43Z</td>\n",
       "      <td>30.269103</td>\n",
       "      <td>-97.749395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316637</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-17T23:42:03Z</td>\n",
       "      <td>30.255731</td>\n",
       "      <td>-97.763386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16516</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-17T19:26:05Z</td>\n",
       "      <td>30.263418</td>\n",
       "      <td>-97.757597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535878</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-16T18:50:42Z</td>\n",
       "      <td>30.274292</td>\n",
       "      <td>-97.740523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user         check-in time   latitude  longitude\n",
       "location id                                                  \n",
       "22847           0  2010-10-19T23:55:27Z  30.235909 -97.795140\n",
       "420315          0  2010-10-18T22:17:43Z  30.269103 -97.749395\n",
       "316637          0  2010-10-17T23:42:03Z  30.255731 -97.763386\n",
       "16516           0  2010-10-17T19:26:05Z  30.263418 -97.757597\n",
       "5535878         0  2010-10-16T18:50:42Z  30.274292 -97.740523"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_checkins.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan to addres this problem wil be:\n",
    "\n",
    "-  <b> <em> Preprocess the raw-data: </em></b></i> <p style=\"text-align:justify\">let's deletete the information that there is not important for our challenge, (<b><em>latitude, longitude</em></b>). in each step it will shown how change the size of the data. I will create a Sparse matrix Object to represent the matrices. we will have 3 matrix: friend_relationship, stalker_score, locations_chekins. for better explanation, in each step there will be a little discription about what I'm doing.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cleaning of  \"latitude\" and \"longitude\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def size_mb(x):\n",
    "    \"\"\"return the size in megabytes of one obeject\"\"\"\n",
    "    \n",
    "    bytes_size = sys.getsizeof(x)\n",
    "    \n",
    "    return (bytes_size /(1024*1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of checkins raw_dataset: 669.74 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of checkins raw_dataset: %.2f MB\" %  size_mb(data_checkins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Friendship network raw_dataset: 29.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Friendship network raw_dataset: %.2f MB\" %  size_mb(data_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimination of latitude and longitude columns\n",
    "data_checkins.drop(['latitude', 'longitude'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of checkins without latitude and longitude: 571.43 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of checkins without latitude and longitude: %.2f MB\" %  size_mb(data_checkins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Pre-processing check-in time\n",
    "Now the date is type string, let's change it to python date type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_checkins['check-in time'] = pd.to_datetime(data_checkins['check-in time'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>check-in time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8904</th>\n",
       "      <td>343</td>\n",
       "      <td>2009-08-21 03:01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8904</th>\n",
       "      <td>576</td>\n",
       "      <td>2009-10-05 20:19:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8904</th>\n",
       "      <td>24</td>\n",
       "      <td>2009-02-05 06:27:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8904</th>\n",
       "      <td>310</td>\n",
       "      <td>2009-12-06 22:51:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8904</th>\n",
       "      <td>5164</td>\n",
       "      <td>2010-01-25 01:29:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user       check-in time\n",
       "location id                          \n",
       "8904          343 2009-08-21 03:01:19\n",
       "8904          576 2009-10-05 20:19:24\n",
       "8904           24 2009-02-05 06:27:43\n",
       "8904          310 2009-12-06 22:51:49\n",
       "8904         5164 2010-01-25 01:29:46"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_checkins.sort_values(by=['location id'], inplace = True)\n",
    "data_checkins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of checkins with date pre-processed: 147.47 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of checkins with date pre-processed: %.2f MB\" %  size_mb(data_checkins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\"> We could see how just adjusting the datatype of the <b><em>\"check-in time\"</em></b> we achieve a huge reduction of the size of our principal dataset.\n",
    "\n",
    "Let's check if there is some missing value\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Checking missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user             0\n",
       "check-in time    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_checkins.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user      0\n",
       "friend    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_edges.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> There is not missing values in both datasets, so we can continue.\n",
    "\n",
    "I think we can reduce even more the size of the datasets.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 sparse matrix\n",
    "\n",
    "<p style=\"text-align:justify\"> To solve our challenge we can implemnt sparse matrices, with help of dicts.</p> \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMatrix:\n",
    "    \"\"\" \n",
    "    \n",
    "    Object for manage big matrices\n",
    "    \n",
    "    based in python dicts and list, \n",
    "    list just for unidimensionals\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__matrix = {} #private attribute\n",
    "    \n",
    "    def get_row(self,row_id):\n",
    "        if row_id in self.__matrix:\n",
    "            return self.__matrix[row_id]\n",
    "        return False;\n",
    "    \n",
    "    def element_exist(self, row_id, col_id):\n",
    "        try:\n",
    "            if not row_id in self.__matrix:\n",
    "                return False\n",
    "\n",
    "            if col_id in self.get_row(row_id):\n",
    "                return True\n",
    "            return False\n",
    "        except ValueError:\n",
    "            print(\"you are using a unidimesional version, this version not has col_id\") \n",
    "    \n",
    "    def get_val(self, row_id, col_id):\n",
    "        try:\n",
    "            return self.__matrix[row_id][col_id]\n",
    "        except ValueError:\n",
    "            print(\"you are using a unidimesional version, this version not has col_id\") \n",
    "        #return self.__matrix[row_id][col_id]\n",
    "    \n",
    "    def get_index(self):\n",
    "        return self.__matrix.keys()\n",
    "    \n",
    "    def get_matrix(self):\n",
    "        return self.__matrix\n",
    "    \n",
    "    def append(self, k, row_id, col_id = None):\n",
    "        \"\"\"\n",
    "        add a new item.\n",
    "        \n",
    "        k could be any type of data.\n",
    "        \"\"\"\n",
    "        if col_id is None:\n",
    "            if  row_id in self.__matrix:\n",
    "                self.__matrix[row_id].append(k)\n",
    "            else:\n",
    "                self.__matrix[row_id] = [k]\n",
    "        else:\n",
    "            if  row_id in self.__matrix:\n",
    "                self.__matrix[row_id].update({col_id: k})\n",
    "            else:\n",
    "                self.__matrix[row_id] = {col_id: k}\n",
    "       \n",
    "    \n",
    "    def update(self, k, row_id, col_id = None):\n",
    "        \"\"\"\n",
    "        update item in the position [row_id, col_id].\n",
    "        \n",
    "        k could be any type of data.\n",
    "        \"\"\"\n",
    "        \n",
    "        if col_id is None:\n",
    "            self.__matrix[row_id] = k\n",
    "        else:\n",
    "            self.__matrix[row_id][col_id] = k \n",
    "        \n",
    "        \n",
    "    def concat(self, Matrix):\n",
    "        self.__matrix.update(Matrix.get_matrix())\n",
    "    \n",
    "    def pprint(self, k = 4):\n",
    "        \"\"\" \n",
    "        print the rows of the matrix\n",
    "        \n",
    "        by defect print the first 5 \n",
    "        \"\"\"\n",
    "        \n",
    "        counter = 0\n",
    "        for x, y in self.__matrix.items():\n",
    "            print(f'{x} -> {y}')\n",
    "            counter += 1\n",
    "            if counter == k :\n",
    "                break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 transforming representation of data\n",
    "\n",
    "<p style=\"text-align:justify\"> Now, that we have our class SparseMatrix we can represent our datasets: Checkins and friendship network, and our result set: stalker_classification, in this data structure. It definetly will reduce the size in RAM of our datasets and it will provide more performance in our <b><em>step 3. Processing.</em></b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Transformation of data_checkins\n",
    "\n",
    "<p style=\"text-align:justify\"> For this transformation let's make some asumptions:\n",
    "    <ul>\n",
    "        <li>based on the definition of the challenger: <em>\"stalker is someone who, in this dataset, visits some of the same locations as another person, after the other person goes to that location.\"</em>  we could say that there is not timestamp defined, in this way we could say that if the person <em>A</em> went to the place <em>X</em> the 01/01/2020 and the person <em>B</em> went to the same place the 05/01/2020 B will be considered has stalker of <em>A</em>. It means that while: $$A_{date}(X) < B_{date}(X) \\implies (A,B) = 1 $$\n",
    "this is valid for each place</li>\n",
    "        <li>\n",
    "            Using the previous assumption we could say that for our analysis we only need at most two dates for each user over one place: the first date that the user has visted the place and the last date, obviously there are users that has visited a certain place just one time, in this case we will use just one date. It will reduce the size of our dataset, because there are user that are made chekin more than two times in the same place, for example in their jobs. \n",
    "        </li>\n",
    "        <br>\n",
    "        <li>\n",
    "            if A place x just has been visited by one user, for our challenge is useless, so let delete this places, it will reduce more the size of our pre-procesed dataset\n",
    "        </li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this to measure the time that takes execute a function\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first create our sparse matrix \n",
    "locations_chekins = SparseMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280969\n"
     ]
    }
   ],
   "source": [
    "#lets extract the unique ids of the places\n",
    "location_ids = data_checkins.index.unique()\n",
    "print(len(location_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the time in seconds that has taken pre process the data_checkins\n",
      "2455.653270483017\n"
     ]
    }
   ],
   "source": [
    "#lets calc the time:\n",
    "# Start the stopwatch / counter \n",
    "start = time.time()\n",
    "for location_id in location_ids:\n",
    "    #select the users and dates\n",
    "    location_checkins = data_checkins.loc[location_id]\n",
    "    #ommiting the places with just one visitor\n",
    "    if location_checkins.size > 2:\n",
    "        #lets change the index for the users id\n",
    "        location_checkins.reset_index(inplace = True)\n",
    "        location_checkins.set_index(\"user\", inplace = True)\n",
    "        #lets extract the unique ids of the users that visited the place\n",
    "        user_ids = location_checkins.index.unique()\n",
    "        #ommit locations with just one visitor\n",
    "        if len(user_ids) > 1:\n",
    "            #lets extract the dates in wich each user has visited the place\n",
    "            for user_id in user_ids:\n",
    "                user_dates = location_checkins.loc[user_id]\n",
    "                #more than one visit?\n",
    "                # two because a df with just one register has a dimensionality of 2 \n",
    "                if user_dates.size > 2:\n",
    "                    #extract the first and last date\n",
    "                    first_date = user_dates[\"check-in time\"].min()\n",
    "                    last_date = user_dates[\"check-in time\"].max()\n",
    "                    #lets add this register to our SparseMatrix\n",
    "                    locations_chekins.append([first_date,last_date], location_id, user_id)\n",
    "                else:\n",
    "                    date = user_dates[\"check-in time\"]\n",
    "                    #lets add this register to our SparseMatrix\n",
    "                    locations_chekins.append(date, location_id, user_id)\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "            \n",
    "#cleaning vars\n",
    "del location_id, location_checkins, user_ids, user_id, user_dates, first_date, last_date, date\n",
    "\n",
    "# Stop the stopwatch / counter \n",
    "stop = time.time() \n",
    "print(\"This is the time in seconds that has taken pre process the data_checkins\")\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load / save the pre-processed files\n",
    "def load_obj(path):\n",
    "    pickle_in = open(path,\"rb\")\n",
    "    obj = pickle.load(pickle_in)\n",
    "    return obj\n",
    "\n",
    "def save_obj(obj, path):\n",
    "    #save the preprocessed data\n",
    "    pickle_out = open( path + \".data\",\"wb\")\n",
    "    pickle.dump(obj, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3784"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_obj(locations_chekins, \"output/locations_chekins\")\n",
    "#locations_chekins = load_obj(\"output/locations_chekins.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lets see the size of our dataset after the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the size of our dataset already pre-processed: \n",
      "20.000099182128906\n"
     ]
    }
   ],
   "source": [
    "print(\"this is the size of our dataset already pre-processed: \")\n",
    "print(size_mb(locations_chekins.get_matrix()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> we could see that from our initial size of 700 MB we have reduce it until 20 MB it is a reduction of 97.15%. For the other hand the pre processing has taken arround 43 minutes, let see if we could implement concurrence to reduce this time and get the same result</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1.1 applying concurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this  function for the concurrence\n",
    "def split_list(array, n_splits = 4):\n",
    "    \"\"\" this function split a lsit in almost equal parts\"\"\"\n",
    "    \n",
    "    splits = []\n",
    "    delta = math.floor((len(array)/n_splits))\n",
    "    for i in range(n_splits):\n",
    "        if i == n_splits - 1 :\n",
    "            a = array[(i*delta): ]\n",
    "            splits.append(a)\n",
    "        else:\n",
    "            a = array[(i*delta) : ((i+1)*delta)]\n",
    "            splits.append(a)\n",
    "            \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_preprocessing(loc_ids, data_checkins):\n",
    "    \"\"\" \n",
    "    this function will be executed for the threads\n",
    "    \n",
    "    this function will extract our useful data \n",
    "    of a user that has visted a place \n",
    "    \"\"\"\n",
    "    \n",
    "    temp = SparseMatrix()\n",
    "    \n",
    "    for location_id in loc_ids:\n",
    "        #select the users and dates\n",
    "        location_checkins = data_checkins.loc[location_id]\n",
    "        #ommiting the places with just one visit\n",
    "        if location_checkins.size > 2:\n",
    "            #lets change the index for the users id\n",
    "            location_checkins.reset_index(inplace = True)\n",
    "            location_checkins.set_index(\"user\", inplace = True)\n",
    "            #lets extract the unique ids of the users that visited the place\n",
    "            user_ids = location_checkins.index.unique()\n",
    "            #ommit locations with just one visitor\n",
    "            if len(user_ids) > 1:\n",
    "                #lets extract the dates in wich each user has visited the place\n",
    "                for user_id in user_ids:\n",
    "                    user_dates = location_checkins.loc[user_id]\n",
    "                    #more than one visit?\n",
    "                    # two because a df with just one register has a dimensionality of 2 \n",
    "                    if user_dates.size > 2:\n",
    "                        #extract the first and last date\n",
    "                        first_date = user_dates[\"check-in time\"].min()\n",
    "                        last_date = user_dates[\"check-in time\"].max()\n",
    "                        #lets add this register to our SparseMatrix\n",
    "                        temp.append([first_date,last_date], location_id, user_id)\n",
    "                    else:\n",
    "                        date = user_dates[\"check-in time\"]\n",
    "                        #lets add this register to our SparseMatrix\n",
    "                        temp.append(date, location_id, user_id)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    #cleaning vars\n",
    "    del location_id, location_checkins, user_ids, user_id, user_dates, first_date, last_date, date\n",
    "    return temp\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create the sparse matrix for concurrency\n",
    "concurrency_locations_chekins = SparseMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the time that has taken pre process concurrently the data_checkins\n",
      "1387.2724704742432\n"
     ]
    }
   ],
   "source": [
    "# Start the stopwatch / counter \n",
    "start = time.time()\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    split_loc_ids = split_list(location_ids, 6)\n",
    "    results = [executor.submit(location_preprocessing, split, data_checkins) for split in split_loc_ids]\n",
    "    \n",
    "    for f in concurrent.futures.as_completed(results):\n",
    "        concurrency_locations_chekins.concat(f.result())\n",
    "        \n",
    "# Stop the stopwatch / counter \n",
    "stop = time.time() \n",
    "print(\"This is the time that has taken pre process concurrently the data_checkins\")\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(concurrency_locations_chekins, \"output/concurrency_locations_chekins\")\n",
    "#concurrency_locations_chekins = load_obj(\"output/concurrency_locations_chekins.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the size of our dataset already pre-processed: \n",
      "20.000099182128906\n"
     ]
    }
   ],
   "source": [
    "print(\"this is the size of our dataset already pre-processed: \")\n",
    "print(size_mb(concurrency_locations_chekins.get_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dont need more this raw dataset\n",
    "del data_checkins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that we could have the same result in almost the middle of the time, i think that whit a graphic card it could be even faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 transforming  data_edges\n",
    "<p> Like the data_checkins this information will be stored in a sparse matrix. We could say that, for be a non directed graph,  if $u_0$ is friend of $u_1$ so $u_1$ is friend of $u_0$. $$ if (u_0,u_1) \\implies (u_1,u_0)$$ In this way and for use less space we could sort ascendig the user_id, and store the friendship relation  between ($u_0$, $u_1$) in the user list that has the smallest id.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>friend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      friend\n",
       "user        \n",
       "0          1\n",
       "0          2\n",
       "0          3\n",
       "0          4\n",
       "0          5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_edges.set_index(\"user\", inplace=True)\n",
    "#sort the values is important for the concurrency that we will apply\n",
    "data_edges.sort_index(inplace=True)\n",
    "data_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of raw data_edges: 29.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of raw data_edges: %.2f MB\" %  size_mb(data_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let see the number of unique users\n",
    "users = data_edges.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196591\n"
     ]
    }
   ],
   "source": [
    "print(len(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges_preprocessing(user_ids, data_edges):\n",
    "    \"\"\"\n",
    "    this function will be used for concurrency\n",
    "    \n",
    "    it will return a sparese matrix with the \n",
    "    friend list of a certain number of users. \n",
    "    \"\"\"\n",
    "    temp = SparseMatrix()\n",
    "    for user_id in user_ids:\n",
    "        #friend of the user_id\n",
    "        user_friends = data_edges.loc[user_id]\n",
    "        #list of frieds\n",
    "        if user_friends.size > 1:\n",
    "            friend_list = user_friends[\"friend\"].values\n",
    "\n",
    "            for friend in friend_list:\n",
    "                # we will just store te friendship of te user that\n",
    "                # have an id bigert than the user_id\n",
    "                if user_id < friend:\n",
    "                    # friend = value, userd_id = row_id\n",
    "                    temp.append(friend, user_id)\n",
    "        else:\n",
    "            unique_friend = user_friends.friend\n",
    "            if user_id < unique_friend:\n",
    "                temp.append(unique_friend, user_id)\n",
    "        \n",
    "    return temp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this matrix will be unidimensional\n",
    "concurrency_friendship = SparseMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the time that has taken pre process concurrently the data_edges\n",
      "35.17928957939148\n"
     ]
    }
   ],
   "source": [
    "# Start the stopwatch / counter \n",
    "start = time.time()\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    split_user_ids = split_list(users, 6)\n",
    "    results = [executor.submit(edges_preprocessing, split, data_edges) for split in split_user_ids]\n",
    "    \n",
    "    for f in concurrent.futures.as_completed(results):\n",
    "        concurrency_friendship.concat(f.result())\n",
    "        \n",
    "# Stop the stopwatch / counter \n",
    "stop = time.time() \n",
    "print(\"This is the time that has taken pre process concurrently the data_edges\")\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(concurrency_friendship, \"output/concurrency_friendship\")\n",
    "#concurrency_friendship = load_obj(\"output/concurrency_friendship.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the size of our data_edges dataset already pre-processed: \n",
      "5.000099182128906\n"
     ]
    }
   ],
   "source": [
    "print(\"this is the size of our data_edges dataset already pre-processed: \")\n",
    "print(size_mb(concurrency_friendship.get_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't need more the dataset \n",
    "del data_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Almost 84% of size reduction and processed in just  36 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Processing the Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Stalker score\n",
    "\n",
    "<p>let's remeber:</p>\n",
    "\n",
    "<ul>\n",
    "  <li>A “stalker score” for a pair of people, A &amp; B, is the number of locations for which A has visited a location followed by B visiting that same location in the future.</li>\n",
    "  <li>Any given location should be counted once in the score, so a stalker score can never be higher than the number of unique locations that A and B have in common.</li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "    We will represent the result as a Sparse Matrix, the row_id will be the user that has been stalked (A), the col_id is te user that stalks (B), and the value is in how many locations the behaviour is the same. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stalker_score = SparseMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the time that has taken calc the stalker score\n",
      "237.78892278671265\n"
     ]
    }
   ],
   "source": [
    "# Start the stopwatch / counter \n",
    "start = time.time()\n",
    "\n",
    "# for each location id in the sparse matrix\n",
    "for location_id in concurrency_locations_chekins.get_index():\n",
    "    #select the user-chekins of this locations\n",
    "    users_location_checkins = concurrency_locations_chekins.get_row(location_id)\n",
    "    # for speed up this coputation lets sort the checkins by user_id\n",
    "    users = sorted(users_location_checkins.keys())\n",
    "    # we could avoid the last computation\n",
    "    # wen the process arrives to the las user\n",
    "    # for him everything will be caculated.\n",
    "    for user in users[:-1]:\n",
    "        dates = users_location_checkins[user]\n",
    "        #if is list there is a first and last date\n",
    "        if type(dates) == list:\n",
    "            current_first = dates[0]\n",
    "            current_last = dates[1]\n",
    "        #if not there is just one date as first and last \n",
    "        else:\n",
    "            current_first = dates\n",
    "            current_last = dates\n",
    "        for next_user in users:\n",
    "            if next_user > user:\n",
    "                next_user_dates = users_location_checkins[next_user]\n",
    "                if type(next_user_dates) == list:\n",
    "                    next_first = next_user_dates[0]\n",
    "                    next_last = next_user_dates[1]\n",
    "                else:\n",
    "                    next_first = next_user_dates\n",
    "                    next_last = next_user_dates\n",
    "                # double way comparison \n",
    "                # if the first date of the current user is before the lastone date of the next user\n",
    "                # next is stalking current\n",
    "                # has to be just smallest comparison because the challene say in the future\n",
    "                if current_first < next_last:\n",
    "                    if stalker_score.element_exist(user, next_user):\n",
    "                        score = stalker_score.get_val(user, next_user) \n",
    "                        stalker_score.update(score + 1, user, next_user)\n",
    "                    else:\n",
    "                        stalker_score.append(1, user, next_user)\n",
    "                # if the first date of next user is before the lastone date of the curren user\n",
    "                # current user is stalking the nextuser\n",
    "                if next_first < current_last:\n",
    "                    if stalker_score.element_exist(next_user, user):\n",
    "                        score = stalker_score.get_val(next_user, user) \n",
    "                        stalker_score.update(score + 1, next_user, user)\n",
    "                    else:\n",
    "                        stalker_score.append(1, next_user, user)\n",
    "                else:\n",
    "                    # is not a stalker\n",
    "                    pass\n",
    "            else:\n",
    "                # you can't stalk yourself\n",
    "                pass\n",
    "            \n",
    "# Stop the stopwatch / counter \n",
    "stop = time.time() \n",
    "print(\"This is the time that has taken calc the stalker score\")\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(stalker_score, \"output/stalker_score\")\n",
    "#stalker_score = load_obj(\"output/stalker_score.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the size of stalker_score: \n",
      "5.000099182128906\n"
     ]
    }
   ],
   "source": [
    "print(\"this is the size of stalker_score: \")\n",
    "print(size_mb(stalker_score.get_matrix()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Friend and not friend highest stalker score\n",
    "\n",
    "<p style=\"text-align:justify\"> In this step we must to check all the positions to find the highest score, but for speed up this process we will just have in count the stalkers' scores bigguer than the currently calculated. For speed up this search we will sort the rows of our matrix by stalker score value </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the time that has taken find te highest scores\n",
      "14.482219219207764\n"
     ]
    }
   ],
   "source": [
    "stalked_user_list = stalker_score.get_index()\n",
    "\n",
    "# we will save the position in the stalker score matrix in two dict\n",
    "friends_coordinates = {}\n",
    "friends_score = 0\n",
    "not_friends_coordinates ={}\n",
    "not_friends_score = 0\n",
    "\n",
    "# Start the stopwatch / counter \n",
    "start = time.time()\n",
    "\n",
    "for stalked_user in stalked_user_list:\n",
    "    # get the user that have stalked at stalked_user\n",
    "    stalkers_list = stalker_score.get_row(stalked_user)\n",
    "    \n",
    "    # Create a list of tuples sorted by index  \n",
    "    list_tuples = sorted(stalkers_list.items() , reverse=True, key=lambda x: x[1])\n",
    "    \n",
    "    # Iterate over the sorted sequence\n",
    "    for stalker, score in list_tuples :\n",
    "        if score > friends_score or score > not_friends_score:\n",
    "            #this is for verify is they are friends \n",
    "            #we store the friendship in the user wish id is the smallest one \n",
    "            if stalked_user < stalker:\n",
    "                # get the friend of the staleked user\n",
    "                stalked_friends_list = concurrency_friendship.get_row(stalked_user)\n",
    "                if stalked_friends_list and stalker in stalked_friends_list:\n",
    "                    if score > friends_score:\n",
    "                        friends_coordinates = {stalked_user : stalker}\n",
    "                        friends_score = score\n",
    "                else:\n",
    "                    if score > not_friends_score:\n",
    "                        not_friends_coordinates = {stalked_user : stalker}\n",
    "                        not_friends_score = score\n",
    "            else:\n",
    "                # get the friend of the staleker user\n",
    "                stalker_friends_list = concurrency_friendship.get_row(stalker)\n",
    "                if stalker_friends_list and stalked_user in stalker_friends_list:\n",
    "                    if score > friends_score:\n",
    "                        friends_coordinates = {stalked_user : stalker}\n",
    "                        friends_score = score\n",
    "                else:\n",
    "                    if score > not_friends_score:\n",
    "                        not_friends_coordinates = {stalked_user : stalker}\n",
    "                        not_friends_score = score\n",
    "        else:\n",
    "            #he found the not interesting values\n",
    "            break\n",
    "\n",
    "stop = time.time() \n",
    "print(\"This is the time that has taken find te highest scores\")\n",
    "print(stop - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Presentation of the results:\n",
    "<p> To understand the resut, the first user has been stalked for the second a total time of the stalker score, i.e. the user <em>10410</em> has been stalked for his friend, the user <em>10393</em> with a total of 365 times in different locations</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Couple of Friends with the highest score: {10410: 10393} ; stalker score: 365\n"
     ]
    }
   ],
   "source": [
    "print(f' Couple of Friends with the highest score: {friends_coordinates} ; stalker score: {friends_score}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Not friends: {1251: 106819} ; stalker score: 388\n"
     ]
    }
   ],
   "source": [
    "print(f' Not friends: {not_friends_coordinates} ; stalker score: {not_friends_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
